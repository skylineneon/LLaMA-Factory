services:
  llamafactory:
    build:
      dockerfile: ./docker/docker-cuda/Dockerfile
      context: ../..
      args:
        INSTALL_BNB: false
        INSTALL_VLLM: false
        INSTALL_DEEPSPEED: false
        INSTALL_FLASHATTN: false
        INSTALL_LIGER_KERNEL: false
        INSTALL_HQQ: false
        INSTALL_EETQ: false
        PIP_INDEX: https://pypi.org/simple
        EXTRAS: metrics
    container_name: llamafactory
    volumes:
      - ../../hf_cache:/root/.cache/huggingface
      - ../../ms_cache:/root/.cache/modelscope
      - ../../om_cache:/root/.cache/openmind
      - ../../data:/app/data
      - ../../output:/app/output
      - /DATA/LLM/gaojiale/llm_project/llama_factory/model/:/app/model
      - /DATA/LLM/gaojiale/llm_project/llama_factory/LLaMA-Factory/examples:/app/examples
      - /DATA/LLM/gaojiale/llm_project/llama_factory/LLaMA-Factory/docker/docker-cuda/start_llamafactory.sh:/app/start_llamafactory.sh
    ports:
      - "7860:7860"
      - "8000:8000"
    ipc: host
    tty: true
    shm_size: '16gb'
    stdin_open: true
    command: ['/bin/bash', '/app/start_llamafactory.sh']  #在启动容器时执行脚本,注意路径
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            count: "all"
            capabilities: [ gpu ]
    restart: unless-stopped
